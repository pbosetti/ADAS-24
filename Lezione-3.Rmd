---
title: "Lezione 3: Regressione"
author: "Paolo Bosetti"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    number_sections: true
  html_notebook:
    toc: true
  word_document:
    toc: true
  html_document:
    toc: true
header-includes: \usepackage[italian]{babel}
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(modelr)
library(ROCR)
source("my_lib.R")

knitr::opts_chunk$set(
  fig.dim=c(5,3),      # dimensioni delle figure in pollici
  out.width = "10cm",  # larghezza figure sul documento
  fig.align = "center"
) 
```

# Regressione lineare

## Modello lineare univariato

Cominciamo con il caso più semplice: la regressione di un modello *univariato*, cioè che dipende da un'unica variabile aleatoria $x$. Ci generiamo i dati corrispondenti alla funzione polinomiale di secondo grado

$$
y_i = bx_i + cx_i^2 + \varepsilon_i
$$

dove $\varepsilon_i \sim \mathcal N(0, \sigma^2)$. Creiamo un data frame per 100 osservazioni:

```{r}
set.seed(0)
N <- 100

df <- tibble(
  x = seq(-10, 10, length.out = N),
  y_nom = 2 * x + 0.1 * x^2,
  y = y_nom + rnorm(N, 0, 2)
)

df %>% 
  ggplot(aes(x=x, y=y)) +
  geom_point() +
  geom_line(aes(y=y_nom), color="red")
```

> Nel grafico, la curva rossa rappresenta il modello nominale $y=bx + cx^2$.

Nella realtà, il modello nominale sarebbe incognito, e potremmo solo osservare che i dati manifestano una certa curvatura, compatibile con un comportamento parabolico. Regrediamo il modello lineare di secondo grado:

$$
y_i = a + b x_i + cx_i^2 + \varepsilon_i
$$ In R, tale modello si traduce nella formula `y~x + I(x^2)`.

> **ATTENZIONE** --- le *formule* si ottengono dai modelli eliminando i residui, il termine costante, e i coefficienti dei fattori. Tuttavia, la formula `y~x+x^2` sarebbe scorretta, infatti secondo l'algebra delle formule si espanderebe come `y~x+(x*x)`, cioè `y~x+(x+x+x:x)`, ma l'interazione `x:x` è ovviamente inefficace, e `x+x+x` corrisponde a `x` (perché sul modello risulterebbe $y_i=3bx_i$, in cui il coefficiente da individuare è comunque solo $c$). 
> Quindi, scrivere `y~x+x^2`sarebbe come scrivere `y~x`, cioè un modello di primo grado. Per evitare questo limite, R dispone della *funzione identità*, `I()`, che protegge il suo argomento e ne forza un'interpretazione letterale.

```{r}
(df.lm <- lm(y~x+I(x^2), data=df))
```

I tre termini `Coefficients` sono ovviamente i coefficienti $a$, $b$, e $c$. Si noti che i termini di primo e secondo grado sono molto vicini a quelli nominali (rispettivamente 2 e 0.1), ma compare anche un termine di grado 0.

La funzione `summary()` può aiutarci a migliorare la regressione:

```{r}
summary(df.lm)
```

Osserviamo che il *p-value* associato all'intercetta è elevato: ciò significa che quel termine contribuisce poco al modello e può quindi essere rimosso. Notiamo inoltre il valore di $R^2$: `r summary(df.lm)$r.squared`.

Rivediamo il modello rimuovendo l'intercetta (cioè forzando $a=0$). Nel linguaggio delle formule, ciò si ottiene **sottraendo 1**:

```{r}
df.lm <- lm(y~x + I(x^2) - 1, data = df)
summary(df.lm)
```

Ora non solo tutti i termini risultano significativi, ma $R^2$ è anche aumentato (vale `r summary(df.lm)$r.squared`).

Confrontiamo ora in grafico la regressione col modello nominale:

```{r}
df %>% 
  add_residuals(df.lm) %>% 
  add_predictions(df.lm) %>% 
  ggplot(aes(x=x, y=y)) +
  geom_point() +
  geom_line(aes(y=y_nom), color="red") +
  geom_line(aes(y=pred), color="darkgreen")
```

Se tutto ciò che interessa è il grafico, è possibile usare la geometria `geom_smooth()` per aggiungere direttamente a un grafico a dispersione la curva di regressione e la **banda di confidenza**. 

```{r}
df %>% 
  ggplot(aes(x=x, y=y)) +
  geom_smooth(
    method="lm",                  # tipo di modello: lm()
    formula = df.lm$call$formula, # riuso la formula di df.lm
    level = 0.99                  # livello di confidenza (default 0.95)
  ) +
  geom_point()
```

Questo è un metodo rapido ma ha lo svantaggio di non fornire né i valori dei coefficienti, né i residui. Questi ultimi invece mi servono perché devo sempre ricordarmi di verificarne la normalità e l'assenza di *pattern*:

```{r}
shapiro.test(df.lm$residuals)

df %>% 
  add_residuals(df.lm) %>% 
  ggplot(aes(sample=resid)) +
  geom_qq() +
  geom_qq_line()

df %>% 
  add_residuals(df.lm) %>% 
  ggplot(aes(x=x, y=resid)) +
  geom_point()
```

Osservo che i residui appaiono normali e privi di pattern, quindi posso accettare la regressione in `df.lm`.


## Estrapolazione

Estrapolare un modello regredito significa valutarlo all'esterno dell'intervallo di predittori sui quali è stato adattato.
Facciamo un esempio: dai nostri dati iniziali selezioniamo solo quelli nell'intervallo $(-7.5, 7.5)$, adattiamo il modello su questi dati e lo confrontiamo con cosa succede all'esterno di tale intervallo:

```{r}
df <- df %>% 
  mutate(
    subset = ifelse(x> -7.5 & x < 7.5, "in", "out")
  )

df %>% 
  ggplot(aes(x=x, y=y, color=subset)) + 
  geom_point() +
  geom_smooth(
    data = filter(df, subset == "in"),
    method = "lm",
    formula = y~x+I(x^2),
    fullrange = TRUE
  ) +
  coord_cartesian(ylim=c(-30, 30))
```

Osserviamo anzitutto che le bande di confidenza si allargano sensibilmente all'esterno dell'intervallo. Questo effetto è molto più evidente se il modello soffre di *sovra-adattamento*: 

```{r}
df <- df %>% 
  mutate(
    subset = ifelse(x> -7.5 & x < 7.5, "in", "out")
  )

df %>% 
  ggplot(aes(x=x, y=y, color=subset)) + 
  geom_point() +
  geom_smooth(
    data = filter(df, subset == "in"),
    method = "lm",
    formula = y~poly(x, 10, raw=T),
    fullrange = TRUE
  ) +
  coord_cartesian(ylim=c(-30, 30))
```

Abbiamo usato la formula `y~poly(x, 10, raw=T)`, che indica un polinomio di grado 10. È evidente che in estrapolazione il modello non rappresenta correttamente i dati e che sull'intervallo di regressione cerca di inseguire i dati **perdendo di generalità**.


## Bande di confidenza e di predizione

La funzione `predict()` consente di ottenere un data frame con le bande di confidenza o di predizione:

```{r}
predict(df.lm, interval = "confidence") %>% head()
```

Questo data frame può essere affiancato a quello originario consentendomi di mettere in grafico le bande (con `geom_ribbon()`):

```{r}
df %>% 
  bind_cols(predict(df.lm, interval="confidence", level=0.999)) %>% 
  ggplot(aes(x=x, y=fit)) + 
  geom_line() +
  geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=0.5) +
  geom_point(aes(y=y))
```

Provare a sostituire `confidence` con `prediction` e discutere la differenza.


# Cross-validazione

La scelta del modello da regredire deve essere **validata**, cioè bisogna verificare che non soffra di sotto-adattamento né di sovra-adattamento. 

Carichiamo un set di dati di esempio:

```{r message=FALSE, warning=FALSE}
df <- read_csv(example_url("kfold.csv"))
df %>% 
  ggplot(aes(x=x, y=y)) +
  geom_point()
```

Proviamo con un modello di primo grado e valutiamo i residui:

```{r}
df.lm <- lm(y~x, data=df)

df %>% 
  add_residuals(df.lm) %>% 
  ggplot(aes(x=x, y=resid)) + geom_point()

```

L'andamento sinuoso significa che il modello non è sufficientemente complesso e che dobbiamo aumentarne il grado. Ma di quanto? Per capirlo utilizziamo la **validazione incrociata** (*cross-validation*), cioè dividiamo i dati in due sottoinsiemi: uno, più grande, usato per la regressione; uno, più piccolo, per la validazione, cioè per la verifica del modello.

```{r message=FALSE, warning=FALSE}
set.seed(1)
df <- df %>% 
  mutate(train = runif(n()) > 1/4)

df %>% 
  ggplot(aes(x=x, y=y, color=train)) +
  geom_point()
```

Costruisco una serie di modelli polinomiali dal grado 1 fino al grado 8 applicato **solo sui punti di addestramento** (`train`) e li metto in una lista:

```{r}
models <- df %>% 
  filter(train) %>% {
    list(
      lm(y~x, data=.),
      lm(y~poly(x, 2, raw=T), data=.),
      lm(y~poly(x, 3, raw=T), data=.),
      lm(y~poly(x, 4, raw=T), data=.),
      lm(y~poly(x, 5, raw=T), data=.),
      lm(y~poly(x, 6, raw=T), data=.),
      lm(y~poly(x, 7, raw=T), data=.),
      lm(y~poly(x, 8, raw=T), data=.)
    )
  }
```

Mi costruisco una tibble con i valori di $R^2$ per ogni modello e li confronto su un grafico a barre:

```{r}
tibble(
  n = 1:length(models),
  train = models %>% map_dbl(~ rsquare(., filter(df, train))),
  valid = models %>% map_dbl(~ rsquare(., filter(df, !train)))
) %>% 
  pivot_longer(cols=c(train, valid), names_to = "type", values_to = "rsquare") %>% 
  ggplot(aes(x=n, y=rsquare, fill=type)) +
  geom_col(position="dodge")
```

Per il dataset di addestramento $R^2$ cresce monotonicamente all'aumentare del grado del polinomio, mentre per il dataset di validazione (verde) raggiunge un massimo al grado 3, poi decresce.

Questo andamento è ancora più evidente se usiamo un altro indicatore della qualità della regressione, cioè l'*errore quadratico medio* RMSE, che ha un andamento opposto (va minimizzato):

$$
\mathrm{RMSE} = \sqrt{\frac{\sum_{i=1}^n (y_i - \hat y_i)^2}{n}}
$$

```{r}
tibble(
  n = 1:length(models),
  train = models %>% map_dbl(~ rmse(., filter(df, train))),
  valid = models %>% map_dbl(~ rmse(., filter(df, !train)))
) %>% 
  pivot_longer(cols=c(train, valid), names_to = "type", values_to = "rmse") %>% 
  ggplot(aes(x=n, y=rmse, fill=type)) +
  geom_col(position="dodge")
```

Osserviamo che RMSE è minimo per una regressione polinomiale di terzo grado, quindi per evitare sotto- e sofra-adattamento scegliamo un polinomio di terzo grado.

**ESERCIZIO**: effettuare la regressione con un polinomio di terzo grado, metterla in grafico, identificare i coefficienti significativi e i loro valori, e verificare i residui.

```{r}
df.lm <- lm(y~poly(x, 3, raw=T), data=df)
summary(df.lm)
```
```{r}
df.lm <- lm(y~poly(x, 3, raw=T)-1, data=df) # rimuovo il termine di grado 0
summary(df.lm)
df %>% 
  add_predictions(df.lm) %>% 
  ggplot(aes(x=x)) + 
  geom_point(aes(y=y)) + 
  geom_line(aes(y=pred), color="red")

df %>% 
  add_residuals(df.lm) %>% 
  ggplot(aes(sample=resid)) +
  geom_qq() + 
  geom_qq_line(color="red")
```


# Dati multivariati

Supponiamo di avere un processo la cui resa $y$ dipende da due variabili $x_1$ e $x_2$. in questo caso possiamo costruire una regressione del modello $y=f(x_1,x_2)$ con $f()$ una combinazione polinomiale.

Analogamente a quanto fatto nei casi precedenti, cominciamo con il costruirci un data frame di dati nominali. IL corrispettivo multidimensionale di `seq()` (che crea una sequenza di valori) è `expand.grid()` (che crea un data frame con tutte le combinazioni di una serie di sequenze):

```{r}
set.seed(10)
N <- 50

# Funzione nominale
y <- function(x1, x2) 10 - x1 + 0.1*x1^2 + 0.1*(-10*x2 + 1.5*x2^2) + 0.05*x1*x2

# Griglia regolare in x1, x2
dfn <- expand.grid(
  x1 = seq(0, 10, length.out=N),
  x2 = seq(0, 10, length.out=N)
) %>% 
  mutate(y = y(x1, x2))

# Grafico a contorno:
dfn %>% 
  ggplot(aes(x=x1, y=x2, z=y)) +
  geom_contour_filled()
```

Da questo campo scalare vogliamo ottenere delle *misure* ripetute tre volte in 100 posizioni casuali. Per ciascuna misura (simulata) aggiungiamo un disturbo normale a media nulla, ripetendo la misura tre volte (cioè replicando il dataset per tre volte, con disturbi differenti per ciascuna replica):

```{r}
Ns <- 100
rpt <- 3

df <- dfn %>% 
  slice_sample(n = Ns) %>% 
  slice(rep(1:n(), each=rpt)) %>% 
  mutate(y = y + rnorm(n(), 0, range(y)/25))
```

Confrontiamo ora le misue simulate (data frame `df`) con quelle nominali (data frame `dfn`):

```{r}
dfn %>% 
  ggplot(aes(x=x1, y=x2)) +
  geom_contour(aes(z=y, color=after_stat(level)), bins=10) +
  geom_point(data=df, mapping=aes(color=y)) +
  scale_color_viridis_b(n.breaks=8, option="A")
```

Si noti che per cambiare il colore delle curve di livello in funzione del valore di `y` come categorizzato dalle curve stesse si usa la funzione `after_stats()`, che mette a disposizione i valori della geometria stessa *dopo che è stata calcolata*.

Si noti che fin qui abbiamo solo generato i dati in un esperimento simulato. Nel caso reale avremmo solamente i dati corrispondenti alle misure:

```{r}
df %>% 
  ggplot(aes(x=x1, y=x2)) + 
  geom_point(data=dfn, color=grey(0.8), shape=3) +
  geom_point(aes(color=y)) +
  scale_color_viridis_b()
```

dove per riferimento abbiamo aggiunto anche la griglia di dati completi (crocette grige), delle quali nell'esperimento abbiamo campionato soltanto un sottoinsieme casuale di `r Ns` punti, ripetuti `r rpt` volte ciascuno.

A questo punto costruiamo un modello lineare completo parabolico in `x1` e `x2`, **comprese tutte le interazioni** (cioè i termini $x_1x_2$, $x_1^2x_2$, $x_1x_2^2$, e $x_1^2x_2^2$):

```{r}
df.lm <- lm(y~poly(x1, 2, raw=T) * poly(x2, 2, raw=T), data=df)
summary(df.lm)
```

Osserviamo che le interazioni risultano tutte non significative, quindi potremmo rimuoverle. Tuttavia $x_1x_2$ (cioè in formula `x1:x2`) ha un *p-value* del 17%: in questi casi è opportuno cautelativamente inserire comunque l'interazione e verificare con un secondo modello lineare:

```{r}
df.lm <- lm(y ~ poly(x1, 2, raw=T) + poly(x2, 2, raw=T) + x1:x2, data=df)
summary(df.lm)
```

Come si vede $x_1x_2$ risulta significativa. Quindi possiamo accettare il modello:

$$
\hat y = \mu + a_1x_1 + a_2 x_2 + b_1 x_1^2 + b_2 x_2^2 + c x_1 x_2
$$

Confrontiamo ora in grafico il modello regredito e l'originale modello nominale:

```{r}
dfn %>% 
  add_predictions(df.lm) %>% 
  ggplot(aes(x=x1, y=x2, z=y)) +
  geom_contour_filled(bins=10) +
  geom_contour(aes(z=pred), bins=10)
```

# Regressione lineare genertalizzata

Abbiamo i risultati di un esperimento che valuta il *drop test* di bottiglie di sapone liquido:

```{r}
data <- read_table(example_url("soap_bottles.dat"), comment="#")

data %>% slice_head(n=6) %>% knitr::kable()
```

Il *drop test* verifica la resistenza alla caduta da 1 m di altezza di una bottiglia di sapone liquido in funzione del livello di riempimento (in % del valore nominale). Bottiglie più piene hanno un polmone d'aria (che è comprimibile) più ridotto, e quindi hanno una maggior probabilità di rottura per via della sovrapressione che si crea in seguito all'impatto col suolo. I dati dell'esperimento riportano il livello di riempimento effettivo e il risultato del test per `r length(data$run)`.

L'esperimento è stato condotto impostando il dosatore dell'impianto di imbottigliamento a un livello di riempimento di 99.3% per 200 bottiglie e a 99.6% per altre 200 bottiglie, e misurando poi con un sistema di precisione l'effettivo riempimento prima del test di rottura.

I risultati possiamo osservarli in un istogramma:

```{r}
data %>% 
  ggplot(aes(x=p)) +
  geom_histogram(bins=20, color="black", fill=gray(0.5)) +
  geom_rug(aes(color=OK))
```

È evidente l'**andamento bimodale** dell'istogramma: ci sono cioè due massimi, dato che l'istogramma è la sovrapposizione di due distribuzioni (una a 99.3% di livello atteso, una al 99.6%). Il comando `geom_rug()` aggiunge una "stuoia" (*rug*) di linee in basso, una per ogni osservazione, colorate in funzione del risultato del *drop test*. Come si osserva, le due popolazioni sono parzialmente sovrapposte, ed è quindi il caso tipico in cui si può ottenere un classificatore con una regressione logistica.

Prima di tutto dividiamo il dataset in due classi, una su cui fare la regressione e una su cui fare la validazione, in ragione di 80/20:

```{r}
set.seed(0)
N <-  length(data$run)
ratio <- 0.8
n <- floor(N * ratio)
data$training <- FALSE
data$training[sample(1:N, n)] <- TRUE
data %>% slice_head(n=10)
```

**ESERCIZIO**: ripetere la stessa operazione di suddivisione del dataset utilizzando le funzioni di `dplyr`.

```{r}
data %>% 
  mutate(
    training = sample(c(T,F), n(), prob=c(80, 20), replace=T)
  )
```




Ora possiamo costruire il modello lineare generalizzato (logistico) sul sottoinsieme di training (`family="binomial"` specifica di usare la funzione di collegamento logistica, corrispondente ad una distribuzione binomiale dei residui):

```{r}
data.glm <- glm(OK ~ p, family="binomial", data=filter(data, training))
summary(data.glm)
```

Si noti che R assume come funzione logistica la seguente versione:

$$
\mathrm{logit}(x) = \frac{1}{1+\exp(-px-m))}
$$
che è del tutto equivalente alla:
$$
\mathrm{logit}(x) = \frac{1}{1+\exp(-p(x-x_0))}
$$
definendo $x_0=-m/p$.

Possiamo quindi calcolare la soglia di classificazione come:

```{r}
(x0 <- -data.glm$coefficients[1] / data.glm$coefficients[2])
```

Cioè le bottiglie con un livello di riempimento effettivo superiore a `r x0` tendono a rompersi nel *drop test*, e il numero di **falsi positivi** (bottiglie al di sopra di `x0` che sopravvivono) è uguale al numero di **falsi negativi** (botiglie al di sotto di `x0` che si rompono).

Aggiungiamo le predizioni ai dati originali:

```{r}
data <- data %>% 
  add_predictions(data.glm, type="response")

data %>% slice_head(n=10) %>% knitr::kable()
```

Ora possiamo mettere in grafico e confrontare i risultati del test e le predizioni, sia sui dati di training che su quelli di validazione:

```{r}
data %>% 
  mutate(wrong = OK != (pred > 0.5)) %>% 
  ggplot(aes(x=p, y=pred)) +
  geom_line() +
  geom_point(aes(y=as.numeric(OK), color=wrong)) +
  geom_vline(xintercept = x0, linetype=2) +
  facet_wrap(~training, nrow=2) +
  labs(x="Livello di riempimento", y="predizione", color="pred. errata")
```

Si noti l'uso di `facet_wrap()`: separa su più grafici i dati da visualizzare, suddividendoli per i valori di fattori espressi dal secondo membro di una formula (in questo caso `~training`).

Confrontiamo i conteggi per le varie condizioni (training e validazione, bottiglia rotta o meno, predizione corretta o errata):

```{r}
error_typ <- function(outcome, pred) {
  if (outcome == pred) return("correct")
  if (outcome & !pred) return("false neg")
  if (!outcome & pred) return("false pos")
}

data %>% 
  mutate(
    type = map2_chr(!OK, pred<0.5, ~error_typ(.x, .y))) %>% 
  group_by(training, OK, type) %>% 
  summarise(count=n())
```

Alcune note:

* ho creato una funzione per definire la tabella di verità (o matrice di confusione) per i possibili risultati del test, assumendo che **un positivo sia una bottiglia rotta** (dato che il negativo è sempre la condizione normale, attesa), e quindi un **falso positivo** sia una bottiglia intera che secondo il modello dovrebbe rompersi
* la funzione `error_typ()` non è vettorializzata, quindi per creare una colonna mediante tale funzione devo applicarla elemento per elemento, e uso la funzione `map2_chr()`, cioè una mappa a due valori che restituisce una stringa

Osservando la tabella, in training ho 11 falsi negativi e 12 falsi positivi, come atteso. In validazione ho 6 falsi positivi e 4 falsi negativi: la differenza è minima e quindi **posso accettare il modello**.

Le matrici di confusione possono essere create anche con la funzione `table()`:

```{r}
mc_t <- table(
  Actual = filter(data, training) %>% mutate(positive=!OK) %>% pull(positive),
  Predicted = filter(data, training) %>% pull(pred) < 0.5
)
round(mc_t / sum(mc_t) * 100, 1)
```

Come si vede, il vantaggio delle tabelle è che possono essere facilmente scalate e trasformate, ad esempio, in percentuali.

Per i dati di validazione, come atteso, vale:

```{r}
mc_v <- table(
  Actual = filter(data, !training) %>% mutate(positive=!OK) %>% pull(positive),
  Predicted = filter(data, !training) %>% pull(pred) < 0.5
)
round(mc_v / sum(mc_v) * 100, 1)
```

È possibile anche in un colpo solo (e usando `$` invece di `dplyr::pull()`):

```{r}
table(
  Actual = !data$OK,
  Predicted = data$pred < 0.5,
  Subset = ifelse(data$training, "Training", "Validation")
)
```

```{r}
pred <- prediction(filter(data, training)$pred, filter(data, training)$OK)
perf <- performance(pred, "tnr", "fnr")
plot(perf, colorize=T, print.cutoffs.at=seq(0, 1, 0.2))
```
```{r}
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, print.cutoffs.at=seq(0, 1, 0.2))
```


## Esercizio

Consideriamo il data frame `iris`, che categorizza tre diverse specie di fiori a seconda delle dimensione dei petali e dei sepali. Come si vede, le tre specie sono equi-popolate:

```{r}
iris %>% 
  group_by(Species) %>% 
  summarise(count=n())
```

Aggiungiamo una colonna per il training (80-20), e realizziamo qualche plot:

```{r}
df <- iris %>%
  mutate(
    Sepal = Sepal.Length * Sepal.Width,
    Petal = Petal.Length * Petal.Width,
    Train = sample(c(F,T), n(), prob=c(20, 80), replace=T)
  )  

df %>% 
  ggplot(aes(x=Petal.Width, y=Petal.Length, color=Species, shape=Train)) +
  geom_point()

df %>% 
  ggplot(aes(x=Sepal.Width, y=Sepal.Length, color=Species, shape=Train)) +
  geom_point()
```
Oppure con `GGally`:

```{r message=FALSE}
library(GGally)
ggpairs(iris, columns=1:4, mapping=aes(color=Species))
```

Tralasciamo la specie `setosa` (molto differente) e concentriamoci sulle altre due, costruendo un classificatore logistico per la specie `versicolor`. Osservando che larghezza e lunghezza delle caratteristiche sono direttamente correlate, costruiamo una combinazione e lavoriamo su di essa:



```{r}
df <- df %>% 
  filter(Species != "setosa") %>% 
  mutate(
    Versicolor = as.numeric(Species == "versicolor")
  )

df %>% 
  ggplot(aes(x=Petal, y=Sepal, color=factor(Versicolor))) +
  geom_point()
```
Procedere con realizzare il modello e verificarne l'efficacia con le matrici di confusione.


# Regressione ai minimi quadrati (non lineare)

Consideriamo il caso di un esperimento di indentazione strumentata, in cui un indentatore di forma definita (ad es. un [indentatore Vickers](https://it.wikipedia.org/wiki/Scala_Vickers)) viene premuto sulla superficie di un materiale monitorando il carico in funzione dello spostamento (o del tempo, se lo spostamento avviene a velocità costante). 

Ci si aspetta un andamento piatto finché non c'è contatto, seguito da un ramo di parabola, raccordata in tangenza nel punto di transizione.

Rusulta:

```{r}
f <- function(t, t0=0, bias=0, a=1) {
  b <- -2 * a * t0
  c <-  bias + a * t0^2
  y <- a * t^2 + b * t + c
  return(ifelse(t < t0, bias, y))
}

ggplot() +
  geom_function(fun=f, args=list(bias=1, t0=-1, a=0.5)) +
  xlim(c(-2.5,5)) +
  labs(x="Tempo (s)", y="Carico (N)")
```

Costruiamo i dati con un disturbo normale:

```{r}
set.seed(1)

data <- tibble(
  t = seq(-10, 10, length.out=100),
  yn = f(t, 2.5, 3, 1),
  y = yn + rnorm(length(t), 0, 2)
)

data %>% 
  ggplot(aes(x=t)) +
  geom_point(aes(y=y)) 
  
```

È il caso di una regressione di un *modello fisico*, che però non è lineare (e non è nemmeno analitico, essendo definito per parti).

La regressione non è quindi realizzabile né con `lm`, né con `glm`. È necessario ricorrere alla **regressione ai minimi quadrati** (*Non-linear Least Squares*) con `nlm`:

```{r}
data.nls <- nls(y~f(t, t0, b, a),
                data = data,
                start=list(
                  t0 = 1,
                  b = 34,
                  a = -10
                ),
                trace=T)

summary(data.nls)
```

Si noti che `nlm` accetta i parametri simili a `lm`, con in più la lista `start`, che deve contenere dei valori iniziali ragionevoli per i parametri del modello. Il parametro `trace` stampa invece i singoli passi di ottimizzazione (serve solo per un eventuale debug).

Ora visualizziamo la regressione, cioè i valori predetti dal modello:

```{r}
data %>% 
  add_predictions(data.nls) %>% 
  ggplot(aes(x=t)) +
  geom_point(aes(y=y)) +
  geom_line(aes(y=pred), color="red") 
```

E infine verifichiamo la normalità dei residui:

```{r}
data %>% 
  add_residuals(data.nls) %>% 
  ggplot(aes(sample=resid)) +
  geom_qq() +
  geom_qq_line(color="red")
```
```{r}
data.nls %>% residuals() %>% shapiro.test()
```

