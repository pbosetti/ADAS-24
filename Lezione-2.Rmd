---
title: "Tidyverse"
author: "Paolo Bosetti"
date: "`r Sys.Date()`"
output:
  html_notebook:
    toc: true
  pdf_document:
    toc: true
    number_sections: true
  word_document:
    toc: true
  html_document:
    toc: true
header-includes: \usepackage[italian]{babel}
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
# ATTENZIONE: questo è un blocco di setup: viene eseguito automaticamente PRIMA 
# di eseguire qualunque blocco successivo. Fate la prova: riavviate RStudio, 
# Aprite questo file e eseguite ad esempio il secondo blocco R. Vedrete che 
# questo blocco viene eseguito per primo.

# ATTENZIONE: se cliccate sul menu ingranaggio DOPO aver chiamato questo blocco
# "setup", modificate le "Default chunk options", cioè le opzioni per tutti i 
# blocchi successivi. Quindi se disabilitate la visualizzazione del codice, 
# la disabilitate per tutti i blocchi successivi. Ve ne accorgete perché compare
# qualcosa di simile:
#  knitr::opts_chunk$set(eval = FALSE, include = FALSE)
# Se invece volete personalizzare SOLO QUESTO blocco, inserite a mano le opzioni 
# del blocco, come sopra, oppure rimuovete il nome "setup", cliccate 
# sull'ingranaggio, modificate le impostazioni e aggiungete il nome setup.

# Eseguo lo script con le mie funzioni personalizzate:
library(modelr)
library(adas.utils)

knitr::opts_chunk$set(
  fig.dim=c(5,3),      # dimensioni delle figure in pollici
  out.width = "10cm",  # larghezza figure sul documento
  fig.align = "center"
) 
```

# Tidyverse

`Tidyverse` è una meta-libreria, cioè una collezione di librerie, che trasforma il modo in cui si utilizza R per descrivere algoritmi di manipolazione e analisi dati.

```{r}
library(tidyverse)
```

Caricando la libreria si vede che in effetti si tratta di 9 librerie separate:

1.  `tibble`: data frame evoluti
2.  `readr`: lettura/scritura file
3.  `dplyr`: manipolazione dati
4.  `tidyr`: riorganizzazione di data frame
5.  `ggplot2`: realizzazione grafici
6.  `purrr`: programmazione funzionale (mappe)
7.  `stringr`: manipolazione stringhe
8.  `forcats`: manipolazione fattori
9.  `lubridate`: manipolazione date e intervalli di tempo

Noi vedremo in particolare le prime 6. Ciascuna di queste librerie, se necessario, può essere caricata separatamente, oppure possiamo caricarle tutte come `tidyverse`.

Si noti che il messaggio di conflitto ottenuto al caricamento è atteso e del tutto normale.

## Pipe

Oltre alle 9 librerie sopra elencate, `tidyverse` fornisce l'**operatore *pipe***. Una *pipe* è un *condotto* che passa l'output di una funzione all'input di una seconda funzione. Lo scopo è di rendere il codice più leggibile, evitando funzioni nidificate, e efficiente, evitando eccessive variabili intermedie. Utilizzando una *pipe* si riscrive una generica espressione `f(x, y, ...)` come `x %>% f(y, ...)`. Generalmente la *pipe* è indicata con `%>%`, ma esiste anche la forma alternativa `|>`.

In R standard:

```{r}
round(mean(rnorm(10)), 2)
```

Questa espressione è particolarmente scomoda da leggere (la prima operazione è quella più interna) e prona ad errori (troppe parentesi). Questa sequenza di operazioni può essere resa più chiara ricorrendo a variabili intermedie:

```{r}
v <- rnorm(10, 2)
v.m <- mean(v)
round(v.m, 2)
```

Ma le variabili intermedie sono meno efficienti e possono essere scomode o prone ad errori (riuso di variabili con lo stesso nome).

Con l'operatore *pipe* si evitano questi problemi: le operazioni vengono chiaramente elencate una dopo l'altra in ordine logico, e l'output di ogni funzione diventa **il primo argomento** della funzione successiva:

```{r}
rnorm(10, 2) %>%  
  mean %>% 
  round(2)
```

## Tibble

Le tibble sono versioni evolute dei data frame, compatibili con questi ultimi. Hanno alcuni vantaggi:

1.  sono più facili da creare
2.  sono più robuste
3.  sono più facili da visualizzare

Nella creazione di un data frame non è possibile fare riferimento ad un'altra colonna, ma bisogna appoggiarsi a variabili intermedie:

```{r}
v <- c(1,7,3,10,3,2,2) # variabile intermedia
data.frame(
  A = v,
  B = v^2
)

# Oppure aggiungere una colonna in un secondo momento:
df <- data.frame(A=c(1,7,3,10,3,2,2))
df$B <- df$A^2
```

Creando una tibble, a differenza che nei data frame, è possibile invece che una colonna faccia direttamente riferimento ad una colonna precedente:

```{r}
tbl <- tibble(
  A = 1:10,
  B = A^2
)
```

Mentre un data frame viene sempre stampato per intero (che per data frame molto lunghi può essere un problema), una tibble viene sempre limitata alle prime 10 righe. Se si vogliono più (o meno) righe, bisogna stamparla con la funzione `print` passando l'opzione `n`:

```{r}
tibble(
  A=1:100,
  B=NA
) %>% print(n=20)
```

Si osservi che le colonne passate a `tibble` devono avere tutte la stessa lunghezza oppure lunghezza pari a 1. In quest'ultimo caso il singolo valore viene ripetuto (come per `NA` nell'esempio). Nel caso di `data.frame`, invece, è possibile passare vettori più corti, con lunghezza pari a un sottomultiplo della lunghezza delle colonne più lunghe: in questo caso il vettore più corto viene *riciclato*:

```{r}
data.frame(
  A=1:6,
  B=c("uno", "due")
)
```

Questo automatismo però è pericoloso (perché l'utente non se ne accorge), ed è quindi stato rimosso da `tibble`.

È sempre possibile convertire una tibble in data frame e viceversa:

```{r}
tbl %>% 
  as.data.frame() %>% 
  tail()
df %>% tibble() %>% str()
```

È possibile creare tibble passando i dati per riga invece che per colonna, mediante la funzione `tribble` (*TRansposed tIBBLE*). La prima riga deve contenere i nomi delle colonne preceduti da `~`:

```{r}
tribble(
  ~x, ~y, ~z,
  "a", 1, 10,
  "b", 5, 8,
  "c", 3, 12
)
```

## Input/Output da file

Esistono le versioni *tidy* delle funzioni per leggere e scrivere file di testo:

- `read_csv()`
- `read_csv2()`
- `read_table()`
- `read_fwf()`
- `write_csv()`
- `write_csv2()`
- `write_table()`
- `write_fwf()`

Il principale vantaggio di queste funzioni rispetto alle equivalenti con il punto nel nome (es. `read.table`) è che esse restituiscono direttamente una tibble.

Vedere il cheatsheet qui: <https://rstudio.github.io/cheatsheets/data-import.pdf>.

Il sito <https://paolobosetti.quarto.pub>, nella sezione *Example data*, fornisce alcuni file con dati di esempio che useremo durante il corso. Per semplificarne il download, creiamo il file `my_lib.R` contenente la funzione indicata sul sito e lo carichiamo all'inizio di ogni notebook con il comando `source("my_lib.R")`. A questo punto possiamo caricare un file come tibble in questo modo:

```{r}
read_table(examples_url("cotton.dat"))
```

A quanto pare, l'importazione non è andata a buon fine. Scarichiamo il file grezzo per capire come mai:

```{r}
read_file(examples_url("cotton.dat")) %>%
  str_trunc(200) %>% # Tronco ai primi 200 caratteri
  cat()              # Stampo la stringa risultante
```

Le prime 4 righe in effetti sono commenti che iniziano col carattere `#`, quindi è opportuno specificare il carattere di commento in `read_table`:

```{r}
data <- read_table(examples_url("cotton.dat"), comment = "#")
data
```

## Gestione delle tabella dati: dplyr

La libreria `dplyr` (parte di Tidyverse) fornisce funzioni utili alla manipolazione di tibbe.

Tidiverse contiene alcune tibble di esempio, utili per impararne l'uso. Tra queste la tibble `starwars`:

```{r}
starwars
```

### Filtrare le righe

"Filtrare" significa selezionare solo le righe che rispondono ad alcuni criteri. Ad esempio, solo i personaggi più alti di 180 cm e con capelli castani:

```{r}
starwars %>% 
  filter(height > 180, eye_color=="brown")
```

### Riordinare le righe

Altra operazione comune è riordinare una tabella secondo uno o più criteri:

```{r}
starwars %>% 
  filter(height > 180, eye_color=="brown") %>% 
  arrange(desc(height), mass)
```

I criteri (che devono essere espressioni logiche, quindi non confindere "=" con "==") vengono passati separati da virgole e vengono applicati in sequenza. In questo esempio, cioè i personaggi vengono *prima* ordinati per statura e poi, a pari statura, per massa.

### Selezione di colonne

Per selezionare solo alcune colonne si usa `select`:

```{r}
starwars %>% 
  filter(height > 180, eye_color=="brown") %>% 
  arrange(desc(height), mass) %>% 
  select(pianeta_natale=homeworld, name:hair_color & !height) %>% 
  slice_head(n=5)
```

Gli argomenti di `select` sono:

-   nomi di colonne
-   intervalli di colonne (`name:hair_color`)
-   espressioni logiche (`name:hair_color & !height`)
-   coppie `nuovo_nome = nome_precedente`

### Modificare una colonna

Con `mutate` è possibile modificare una colonna esistente o creare una nuova colonna sulla base di una o più colonne esistenti, mediante espressioni (vettorializzate!):

```{r}
starwars %>% 
  mutate(
    height = height / 100,
    BMI = (mass/height^2) %>% round(1)
  ) %>% 
  relocate(BMI, .after=height) %>% 
  arrange(desc(BMI))
```

Si noti `relocate`, che consente di spostare una colonna prima o dopo un'altra colonna.

### Raggruppamento e sommario

Se vogliamo raggruppare tutte le osservazioni (righe) avente una colonna in comune e applicare una funzione di aggregazione, utilizziamo `group_by` e `summarise`. La prima funzione si limita a preparare la tabella individuando i gruppi, ma non modifica nulla:

```{r}
starwars %>% 
  group_by(species, sex)
```

La seconda funzione crea di fatto i sommari, in questo caso come media dei valori:

```{r}
starwars %>% 
  group_by(species, sex) %>%
  summarise(
    height = mean(height, na.rm=TRUE),
    mass = mean(mass, na.rm=T)
  ) %>% 
  arrange(desc(height))
```

Per inciso, si noti che la funzione `mean` specifica `na.rm=T`, cioè di omettere tutti i valori `NA` nel calcolare la media:

```{r}
mean(c(1, NA, 3), na.rm=T)
```

### Riorganizzazione

Nel linguaggio dell'analisi dati si dice che si preferiscono dati organizzati in maniera *tidy*, cioè **un'osservazione per riga, un osservando per colonna**. Per capire il significato di questa affermazione, consideriamo un'altra tabella di esempio, `relig_income`. In questo caso, le colonne riportano il conteggio di soggetti parte di un campione statistico, raggruppati per religione (righe) e per classe di retribuzione (colonne). Questa tabella **non è *tidy***, dato che lo stesso osservando (conteggio) è distribuito su più colonne:

```{r}
relig_income
```

La tabella può essere resa *tidy* mediante `pivot_longer`:

```{r}
income <- relig_income %>% 
  pivot_longer(
    !religion,            # lavora su tutte le colonne tranne religion
    names_to = "income",  # i nomi colonna finiscono nella colonna "income"
    values_to = "count"   # i valori (conteggi) nella colonna "count"
  )
income
```

La trasformazione inversa si ottiene con `pivot_wider`:

```{r}
income %>% 
  pivot_wider(
    names_from = income,
    values_from = count
  )
```

## Programmazione funzionale e mappatura di funzioni

Gran parte delle funzioni R opera direttamente su vettori, elemento per elemento, quindi generalmente non c'è bisogno di loop. Per i casi in cui sia necessario operare la stessa trasformazione su tutti gli elementi di un vettore e la trasformazione in questione non è vettorializzata, è possibile utilizzare le [funzioni di `purrr`](https://rstudio.github.io/cheatsheets/purrr.pdf).

In generale esiste una famiglia di funzioni il cui nome comincia con `map_` seguito da un suffisso che indica il tipo del vettore generato (`_int`, `_dbl`, `_chr`, ecc.). Il primo argomento (eventualmente passato via pipe) è il vettire su cui operare, il secondo è una funzione di un solo argomento:

```{r}
map_dbl(1:3, function(x) {x*2})
```

La funzione può essere abbreviata, sostituendo `function(x)` con `~`, e `x` con `.`:

```{r}
1:3 %>%  map_dbl(~ .*2) %>% str()
```

Come si vede, il suffisso **garantisce** sul tipo di vettore che si ottiene:

```{r}
1:10 %>% map_int(~ . * 2) %>%  str()
```

Esiste anche la funzione `map` (senza sufisso), che restituisce sempre una **lista**:

```{r}
1:10 %>% map(~ .*2) %>% str()
```

Per fare un esempio d'uso, riprendiamo la tabella `income` (versione *tidy* di `relig_income`). Vogliamo esprimere le quote in percentuale anziché in numero assoluto. Per questo ci servono i totali di individui in ogni categoria.

Calcoliamo la dimensione del campione per ogni religione:

```{r}
income %>% 
  group_by(religion) %>% 
  summarise(total = sum(count))
```

L'uso in sequenza di `group_by` e `summarise` è così comune che le ultime versioni di `dplyr` consentono una semplificazione:

```{r}
totals <- income %>% 
  summarise(total = sum(count), .by=religion)
```

Ora possiamo aggiungere una colonna con la percentuale:

```{r}
income %>% 
  mutate(
    total = map_dbl(religion, ~ totals[totals$religion==.,]$total),
    perc = (count/total*100) %>% round(2)
  )
```

Oppure, in un colpo solo:

```{r}
income %>% 
  mutate(
    perc = map2_dbl(religion, count, ~ (.y/totals[totals$religion == .x,]$total * 100) %>% round(2) )
  )
```

In realtà, l'operazione sopra eseguita consiste di unire due tabelle secondo una colonna comune, che agisce da cerniera. Questa operazione nel linguaggio dei database si chiama *left join*, e la libreria `dplyr` fornisce proprio una funzione equivalente:

```{r}
income %>% 
  left_join(totals, by="religion") %>% # "religion" è la colonna che fa da cerniera per l'unione
  mutate(
    perc = (count/total*100) %>% round(2)
  )
```

Anche in colpo solo, evitando di definire la tabella `totals` e creandola invece al volo:

```{r}
income %>% 
  left_join(summarise(., total=sum(count), .by=religion)) %>% 
  mutate(perc = (count/total*100) %>% round(2))
```

Notare che la pipe passa un oggetto come primo argomento alla funzione successiva, e se si vuole riutilizzare lo stesso argomento in altre posizioni basta usare la variabile speciale `.`, come sopra in `summarise(., ...`.

# Grafici con `ggplot2`

D'ora in avanti non utilizzeremo più le funzioni di plot base di R, ma ricorreremo piuttosto alla libreria `ggplot2` im clusa in `tidyverse`. È una libreria studiata per realizzare grafici di alta qualità evitando soluzioni che sono considerate fuorvianti o poco chiare.

La libreria è basata sulla funzione `ggplot`, che accetta come primo argomento un data frame o una tibble.

Il secondo argomento è chiamato *mapping* e rappresenta l'*estetica*, cioè definisce il ruolo delle varie colonne della tibble (ascissa, ordinata, colore, gruppo, ecc.). Il mapping si crea con la funzione `aes` (che sta per *aesthetics*):

```{r}
tibble(
  v1 = 1:10,
  v2 = v1 ^ 2,
  v3 = v2 + 10
) %>% 
  ggplot(aes(x=v1, y=v2)) 
```

Come si nota, `ggplot` da sola non crea nessun grafico, ma le etichette degli assi sono giuste e l'estensione degli assi stessi è sufficiente a contenere tutti i dati nella tibble.

Per aggiungere serie grafiche si usano comandi di *geometria*, che iniziano con il suffisso `geom_`:

```{r}
tibble(
  v1 = 1:10,
  v2 = v1 ^ 2,
  v3 = v2 + 10
) %>% 
  ggplot(aes(x=v1, y=v2)) +
  geom_line() +
  geom_point(color="red", size=4)
```

Per aggiungere una seconda serie è necessario specificare una differente estetica in una differente geometria. La logica è:

-   le estetiche comuni alle varie serie (ad esempio l'ascissa) si mettono in `ggplot`
-   le estetiche proprie di ciascuna serie (le ordinate) si mettono nelle geometrie

```{r}
df <- tibble(
  t = 1:10,
  v1 = t ^ 2,
  v2 = v1 + 10
) 

df %>% 
  ggplot(aes(x=t)) +
  geom_line(aes(y=v1), color="red") +
  geom_line(aes(y=v2), color="green") +
  labs(x="Tempo", y="Tensione", title="Esempio")
```

In realtà, se i dati sono in formato *tidy* le serie possono essere definite mediante l'estetica stessa, ottenendo automaticamente anche la legenda:

```{r}
df %>% 
  pivot_longer(v1:v2) %>% # rendo la tibble tidy
  ggplot(aes(x=t, y=value)) +
  geom_line(aes(color=name)) +
  geom_point(color="blue") +
  labs(
    x="Tempo (s)",
    y="Tensione (V)",
    title="Misure di tensione",
    color="Sensore"
  )
```

Da notare:

-   estetiche come `color` (e `linewidth`, `linestyle`, `fill`, `shape`, ecc.) possono essere specificate
    -   nella funzione `aes`, e fungono da aggregatori di serie
    -   come argomenti di `geom_`, e si applicano a tutti gli oggetti di quella geometria
-   la funzione `labs` consente di specificare le etichette per assi, grafico *e legenda*

Vogliamo ora realizzare qualche grafico per illustrare i dati di reddito nella tabella `income`. Anzitutto trasformiamo la colonna `income` da stringa in colonna numerica, che si servirà più avanti per poter riordinare i dati: Ci creiamo una tibble di supporto che definisce le corrispondenze numeriche per ogni intervallo di reddito:

```{r}
(values <- tibble(
  income = relig_income %>% select(!religion) %>% names(),
  v = c((1:5)*10, 75, 100, 150, 300, 0)
))
```

Con un *left join* importiamo questi valori nella tabella originale:

```{r}
income2 <- income %>% 
  left_join(values) %>% 
  rename(income_n=v) %>% 
  relocate(income_n, .after=income) %>% 
  left_join(summarise(., total=sum(count), .by=religion)) %>% 
  mutate(
    income = factor(income, ordered=T, levels=values$income),
    perc = (count/total*100) %>% round(2)
  )

income2
```

Si noti che la colonna `income` è stata ridefinita come `factor(income, ordered=T, levels=values$income)`: in questo modo gli intervalli di reddito vengono rappresentati come fattori (cioè variabili categoriche) ordinati (`ordered=T`) secondo la sequenza logica (come in `values$income`).

Possiamo ora realizzare un grafico a barre sovrapposte, filtrando solo le righe per cui `income_n != 0`:

```{r}
income2 %>% 
  filter(income_n != 0) %>%
  ggplot(aes(x=religion, y=perc, fill=income)) +
  geom_col() +
  coord_flip() +
  scale_fill_viridis_d()
```

Il grafico, infine, può essere migliorato ordinando le religioni in funzione di quelle che hanno la categoria dei più ricchi più numerosa. Per ottenere questo risultato cambiamo la sequenza di ordinazione del fattore `religion`:

```{r}
religion_ord <- income2 %>% 
  filter(income_n == 300) %>% # considero solo i più ricchi
  arrange(desc(perc)) %>%     # in ordine decrescente
  pull(religion) %>%          # estraggo solo la colonna religion
  factor(ordered = T)         # trasformo in vettore ordinato

religion_ord
```

Ora trasformo `income2` modificando l'ordine intrinseco del fattore `religion` e rifaccio il grafico:

```{r}
income2 %>% 
  mutate(
    religion = factor(religion, levels=religion_ord, ordered=T)
  ) %>% 
  filter(income_n != 0) %>% 
  ggplot(aes(x=religion, y=perc, fill=income)) +
  geom_col() +
  geom_hline(yintercept=100, linetype=2) +
  coord_flip() +
  scale_fill_viridis_d() +
  labs(
    x="Religione",
    y="Sul totale (%)",
    fill="Reddito"
  )
```

Esercizio: usando un *left join* tradurre i nomi delle religioni nel grafico.

```{r}
income_t <- income2 %>% 
  left_join(
    tibble(
      religion = income2 %>% pull(religion) %>% as.factor() %>% levels(),
      religione = c("Agnostico", "Ateo", "Buddista", "Cattolico", "Non sa/non risponde", "Evangelico", "Hindu",
                    "Protestante nero", "Testimone di Jehovah", "Ebreo", "Protestante", "Mormone", "Musulmano", "Ortodosso",
                    "Altro cristiano", "Altra fede", "Altra religione mondiale", "Non affiliato")
    )
  )

religione_ord <- income_t %>% 
  filter(income_n == 300) %>% # considero solo i più ricchi
  arrange(desc(perc)) %>%     # in ordine decrescente
  pull(religione) %>%          # estraggo solo la colonna religion
  factor(ordered = T)         # trasformo in vettore ordinato

religione_ord


income_t %>% 
  mutate(
    religione = factor(religione, levels=religione_ord, ordered=T)
  ) %>% 
  filter(income_n != 0) %>% 
  ggplot(aes(x=religione, y=perc, fill=income)) +
  geom_col() +
  geom_hline(yintercept=100, linetype=2) +
  coord_flip() +
  scale_fill_viridis_d() +
  labs(
    x="Religione",
    y="Sul totale (%)",
    fill="Reddito"
  )
```


# Statistica inferenziale

## Test di Student

Caso più generale: test a due campioni, a due lati: creiamo due campioni `s1`e `s2` a partire dalla distribuzione normale. I due campioni avranno dimensioni differenti e saranno estratti da popolazioni con valori attesi e varianze *leggermente* differenti:

```{r}
set.seed(321)

m <- c(10.1, 10.2)
s <- c(0.2, 0.1)
n <- c(10, 14)

s1 <- rnorm(n[1], m[1], s[1])
s2 <- rnorm(n[2], m[2], s[2])

ggplot() +
  geom_point(aes(x=1:n[1], y=s1, color="s1")) +
  geom_point(aes(x=1:n[2], y=s2, color="s2")) +
  geom_hline(aes(color="s1", yintercept=mean(s1))) +
  geom_hline(aes(color="s2", yintercept=mean(s2))) +
  labs(x="ordine", y="valore", color="campione")
```

Per prima cosa è necessario effettuare un test della varianza:

```{r}
(vt <- var.test(s1, s2))
```

Il $p$-value pari a `r vt$p.value %>% round(3)` corrisponde ad una probabilità di errore troppo alta per poter scartare l'ipotesi nulla: di conseguenza, concludiamo che **i due campioni provengono da popolazioni con la medesima varianza**, cioè sono campioni omoschedastici.

A questo punto posso procedere col test di Student, specificando opportunamente l'opzione `var.equal` (con una soglia del 5% sulla probabilità d'errore di tipo I):

```{r}
(tt <- t.test(s1, s2, var.equal = (vt$p.value > 0.05), conf.level=0.99))
```

Concludiamo che, con una probabilità d'errore pari a `r tt$p.value %>% round(3)`, i due campioni **non possono provenire dalla stessa popolazione**, cioè che vale $H_1:~\mu1\neq\mu2$.

Il T-test riporta le seguenti informazioni notevoli:

-   la statistica di test $t_0=`r tt$statistic`$
-   i gradi di libertà $n_1+n_2-2=`r sum(n)-2`$
-   il *p*-value `r tt$p.value`
-   i limiti dell'intervallo di confidenza

La coppia di ipotesi effettivamente verificate è:

\begin{align*}
H_0 &: \mu_1-\mu_2 = \mu_0 \\
H_1 &: \mu_1-\mu_2 \neq \mu_0
\end{align*} dove $\mu_0$ è il parametro `mu=0` usato nella funzione `t.test()`.

Come detto sopra, solo un *p*-value piccolo (solitamente almeno minore di 0.05) ci consente di rigettare $H_0$.

L'intervallo di confidenza **è calcolato relativamente a** $\mu_0$: se nella funzione `t.test` il valore del parametro `mu` (con default a `0`) risulta interno all'intervallo, allora si accetta $H_0$ con il livello di confidenza assegnato (`conf.level`, default a 0.95), e viceversa.

### T-test a un lato

Si noti che se per gli stessi campioni si valuta il corrispondente test a un lato, osservando che $\bar s_1 < \bar s_2$:

```{r}
t.test(s1, s2, 
       var.equal=(vt$p.value>=0.05), 
       mu=0,
       alternative = "less",
       conf.level=0.95)
```

si ottiene un $p$-value minore, cioè si può scartare l'ipotesi nulla con più forza: quando cioè si può già escludere uno dei due lati, il test a un lato è **più potente** (nel senso che ha una maggior potenza).

## Box plot

Dal punto di vista grafico, la differenza tra i due campioni `s1` e `s1` può essere visualizzata con un *box-plot*.

Per crearlo, riorganizziamo i dati in modo da utilizzare una tibble. La funzione `geom_boxplot()` vuole un'estetica in cui in ordinata ci siano i valori e in ascissa una variabile categorica da utilizzare come chiave di raggruppamento:

```{r}
df <- tibble(
  i = c(1:n[1], 1:n[2]),
  sample = c(rep("s1", n[1]), rep("s2", n[2])),
  values = c(s1, s2)
) 

df %>% 
  ggplot(aes(x=sample, y=values)) +
  geom_boxplot(varwidth=T)

# La tibble rende anche più semplice realizzare il grafico a dispersione:
df %>% 
  ggplot(aes(x=i, y=values, color=sample)) +
  geom_point()
```

Osserviamo che per entrambi i campioni il box-plot suggerisce che i punti massimi siano delle anomalie. Verifichiamo questa possibilità.

## Anomalie

### Criterio di Chauvenet

Il criterio di Chauvenet non è disponibile in R, ma è facile costruire una funzione che lo implementi:

```{r}
require(glue)

chauvenet <- function(x, threshold=0.5) {
  abs.diff <- abs(x - mean(x)) / sd(x) # vettore delle differenze
  s0 <- max(abs.diff)                  # massima diff.
  i0 <- which.max(abs.diff)            # posizione mass. diff.
  freq <- length(x) * pnorm(s0, lower.tail = F)
  result <- list(
    s0 = s0,
    index = i0,
    value = x[i0],
    reject = freq < threshold
  )
  samp <- deparse(substitute(x)) # restituisce l'espressione passata come argomento 'x'
  print(glue("Chauvenet's criterion for sample {samp}"))
  print(glue("Suspect outlier: {i0}, value {x[i0]}"))
  print(glue("Expected frequency: {freq}, threshold: {threshold}"))
  print(glue("Decision: {d}", d=ifelse(result$reject, "reject it", "keep it")))
  invisible(result)                    # Il risultato resitituito e invisibile
}

cs1 <- chauvenet(s1)
cs2 <- chauvenet(s2)
```

Si osserva che secondo questo criterio entrambi i massimi valori di `s1` e `s2` sono anomali. Decidiamo quindi di eliminarli dai rispettivi vettori, sostituendoli con `NA` (lavoriamo su una coppia dei vettori iniziali):

```{r}
s1_2 <- s1
s2_2 <- s2

if (cs1$reject) s1_2[cs1$index] <- NA
if (cs2$reject) s2_2[cs2$index] <- NA

s1_2
s2_2
```

Realizziamo nuovamente il box-plot, dopo aver rimosso tutte le righe con almeno un `NA` con la funzione `na.omit()`:

```{r}
tibble(
  i = 1:(n[1] + n[2]),
  sample = c(rep("s1", n[1]), rep("s2", n[2])),
  values = c(s1_2, s2_2)
) %>% 
  na.omit() %>% 
  ggplot(aes(x=sample, y=values)) +
  geom_boxplot(varwidth=T)
```

### Test di Grubb

Nella realtà, piùttosto che applicare il test di Chauvenet si preferisce ricorrere al test di Grubb (fornito dalla funzione `grubbs.test()` del pacchetto `outliers`):

```{r}
library(outliers)
grubbs.test(s1)
```

la quale conferma che il punto massimo di `s1` è un'anomalia. Dato che però il test di Grubb non fornisce *l'indice* dell'elemento anomalo, è necessario ricavarlo manualmente come indice dell'elemento col massimo scarto rispetto alla media del campione:

```{r}
which.max(abs(s1 - mean(s1)))
```

# Esercizi

## Anomalie -- 1

Dopo aver eliminato dai campioni `s1` e `s2` gli eventuali punti anomali mediante test di Grubb, verificare con un test di Student se i valori attesi dei due campioni siano uguali o meno.

```{r}
grubbs.test(s1)
grubbs.test(s2)
```
```{r}
s1[which.max(s1)] <- NA

var.test(na.omit(s1) %>% as.numeric(), s2)
t.test(na.omit(s1) %>% as.numeric(), s2, var.equal=T)
```



## Anomalie -- 2

Generare un campione di `r (N<-35)` dati normali con media `r (m<-38.7)` e deviazione standard `r (sd<-3.15)`. Modificare il quindicesimo punto assegnandogli il valore `r (v<-m+3.0*sd)`.

Commentare il risultato applicando sia il test di Chauvenet, sia il test di Grubb.

```{r}
v <- rnorm(35, 38.7, 3.15)
v[15] <- 38.7 + 3 * 3.15
chauvenet(v)
grubbs.test(v)
```
Secondo Chauvenet il punto va rigettato, secondo Grubbs il p-value è del 7% (un po' alto). Si noti che il punto agiunto sta a tre deviazioni standard dalla media: nella distribuzione normale l'intervallo $\pm 3\sigma$ contiene il 99.7% delle osservazioni.


## Test di Student a un campione

Considerare i seguenti dati:

```{r message=FALSE}
read_table(examples_url("diet.dat")) %>% head(n=10)
```

verificare l'ipotesi che i valori della colonna `cTime` quando `diet` è uguale a `D` provengano da una popolazione con media $\mu_0=59$


## Test di Student accoppiato

Considerare i seguenti dati: essi rappresentano delle misure di durezza effettuate con due diversi strumenti (indentatori `Tip1` e `Tip2`). Si ha il sospetto che uno dei due strumenti non sia correttamente calibrato e si vuole verificare l'ipotesi che le 10 misure ottenute col primo strumento abbiano lo stesso valore atteso di quelle ottenute col secondo. Inoltre, non disponendo di materiale su cui verificare la durezza sufficientemente grande per realizzare tutte e venti le impronte, si effettuano due a due le misure di durezza su piccoli campioni (`specimen`), considerati internamente omogenei ma non necessariamente tutti uguali:

```{r message=FALSE}
read_table(examples_url("hardness2.dat"))
```

Che conclusioni possiamo trarre? rifiutiamo $H_0:~\mu_1=\mu_2$? se sì, con che probabilità d'errore? Cosa cambia con i differenti tipi di test di Student? Cosa possiamo dire sulla loro potenza?


## Box plot

Consideriamo i dati nel file esempio `diet.dat`:

```{r message=FALSE}
# Imposto kable in modo che stampi una lineetta al posto di NA
options(knitr.kable.NA = "--")

#Stampo la tabella in forma compatta ("wide")
read_table(examples_url("diet.dat")) %>% 
  group_by(diet) %>%                          # Raggruppo per dieta
  mutate(ripetizione=1:n()) %>%               # Aggiungo colonna con l'ID del singolo test,
                                              # gruppo per gruppo
  select(cTime, diet, ripetizione) %>%        # Seleziono solo le colonne che servono
  pivot_wider(                                # rendo la tabella "larga"
    names_from = diet,                        # - nomi delle colonne
    names_prefix = "dieta ",                  # - prefisso per i nomi
    values_from = cTime                       # - valori nelle celle
  ) %>% 
  knitr::kable(                               # formatto la tabella per output LaTeX
    caption="Tempo di coagulazione (min)"     # - specifica legenda della tabella
  )
```

Discutere quanto segue:

1. realizzare un box-plot in cui le categorie siano rappresentate dalla colonna `diet` e la variabile in studio dalla colonna `cTime`.
2. Studiando l'help in linea della funzione `geom_boxplot`, verificare l'uso dell'opzione `notch` e discutere il grafico risultante.

**Nota**: Il blocco soprastante mostra come rendere in forma compatta e ben formattata una tabella sul report \LaTeX: compilare il documento per vedere il risultato. Provare a disattivare i vari passi per comprenderne lo scopo e l'effetto[^1].

[^1]: Si noti la sintassi `knitr::kable()`, equivalente a caricare la libreria `library(knitr)` e poi usare la funzione `kable()`: quando si usa una sola funzione di una libreria, anziché caricarla completamente è possibile limitarsi a specificarne il nome prima della funzione, seguito da `::`.


# Verifica di normalità
Il test di Student è basato sull'ipotesi che i due campioni siano **indipendenti**. Quest'ipotesi può essere verificata mediante la correlazione o i test di correlazione.

Lo stesso test e molti altri test che abbiamo visto e vedremo sono basati sull'assunzione di normalità del campione o dei residui. Quest'ipotesi può essere verificata con un metodo grafico o con due test statistici.

## Verifica correlazione

Se due campioni non sono correlati, il grafico di uno contro l'altro mostra una nuvola senza tendenze particolari. Viceversa, se c'è correlazione la nuvola appare allungata:

```{r}
set.seed(0)
N <- 50

df <- tibble(
  s1 = rnorm(N, 3, 1),
  s2 = rnorm(N, 5, 1),
  s3 = 2 + 1.5*s1 + rnorm(N, 0, 0.5)
)

df %>% 
  ggplot(aes(x=s1)) +
  geom_point(aes(y=s2), color="red") + 
  geom_point(aes(y=s3), color="green")
```

Quantitativamente possiamo calcolare la covarianza e la correlazione:

```{r}
cov(df$s1, df$s2)
cov(df$s1, df$s3)

cor(df$s1, df$s2)
cor(df$s1, df$s3)
```

La covarianza dà effettivamente poche informazioni, se non che `s1` e `s3` sono effettivamente più correlate di `s1` e `s2`. Più ulile è l'indice di correlazione, sempre compreso tra -1 e 1. Dato che `s1` e `s2` sono effettivamente non correlati, vediamo che anche in casi simili la correlazione non è mai esattamente 0.

R dispone anche di un test di correlazione, con ipotesi nulla di non-correlazione:

```{r}
cor.test(df$s1, df$s2)
cor.test(df$s1, df$s3)
```

I due $p$-value confermano effettivamente che solo `s1` e `s3` sono correlati.


## Normalità

Cominciamo con i metodi grafici: grafico quantile-quantile (*Q-Q plot*), come documentato su <https://paolobosetti.quarto.pub/slides/ADAS/1-statistica.html#/analisi-di-normalit%C3%A0-grafico-quantile-quantile>:

```{r}
set.seed(0)
N <- 10
df <- tibble(
  i = 1:N,
  x = rnorm(N, 10, 2) %>% sort(),
  f = (i-3/8)/(N+1 -3/4),
  q = qnorm(f)
)

# Salvo il graficio per poterlo poi riutilizzare:
plt <- df %>% 
  ggplot(aes(x=q, y=x)) +
  geom_point() +
  labs(x="quantili reali", y="quantili teorici")
plt
```

La verifica va fatta confrontando la posizione dei punti rispetto alla retta di riferimento nominale, che può essere ottenuta come la retta passante per il primo e il terzo quartile:


```{r}
# quantili teorici normali:
qx <- qnorm(c(1, 3)/4)
# quantili del campione:
qy <- quantile(df$x, c(1, 3)/4)
# y = a*x + b
a <- (qy[2] - qy[1])/(qx[2] - qx[1])
b <- qy[1] - a * qx[1]
plt +
  geom_abline(slope = a, intercept = b) +
  geom_point( # uso dati e estetica indipendenti
    mapping=aes(x=x, y=y), 
    data=tibble(x=qx, y=qy), 
    color="red", size=3, shape=3)
```

La libreria `ggplot2` dispone di una funzione apposita per generare direttamente il Q-Q plot. Facciamolo a partire da una tabella *tidy* usando l'estetica `color` per raggruppare due campioni diversi nello stesso grafico:

```{r}
set.seed(123)
N <- 200
df <- tibble(
  sn = rnorm(N, 10, 2),
  su = runif(N, 8, 12)
) %>% 
  pivot_longer(sn:su, names_to="sample", values_to="value")

df %>% 
  ggplot(aes(sample=value, color=sample)) +
  geom_qq() +
  geom_qq_line() +
  labs(x="Quantili teorici", y="Quantili campionari")
```

Osserviamo come anche per un campione effettivamente normale (come `sn`) il grafico Q-Q mostra comunemente delle deviazioni per quantili teorici esterni all'intervallo $[-1,1]$.

Il grafico Q-Q può essere anche utile a individuare rapidamente delle anomalie, che spiccano subito come molto lontane dalla diagonale:

```{r}
df$value[33] <- 50

df %>% 
  ggplot(aes(sample=value, color=sample)) +
  geom_qq() +
  geom_qq_line()
```



## Test di Shapiro-Wilk

Il test di normalità più potente è quello di Shapiro-Wilk. È sempre opportuno accompagnarlo con un'analisi grafica dei dati (es. Q-Q plot), dato che potrebbe indicare una non-normalità anche solo a causa di una o poche anomalie:

```{r}
shapiro.test(df[df$sample == "sn",]$value)
shapiro.test(df[df$sample == "su",]$value)
```

## Test del Chi-quadro

Costruiamo il test secondo [la teoria](https://paolobosetti.quarto.pub/slides/ADAS/1-statistica.html#/analisi-di-normalit%C3%A0-test-del-chi-quadro). Nota l'uso delle funzioni `qnorm()`, `cut()` e `tabulate()`:

```{r}
set.seed(0)

N <- 100
sample <- runif(N, 8, 12)

k <- floor(N/5)
m <- mean(sample)
s <- sd(sample)

b <- qnorm(seq(0, 1, length.out=k+1), m, s)
O <- sample %>% cut(breaks=b) %>% tabulate()
E <- N/k

X0 <- sum((O-E)^2/E)
X0
pchisq(X0, k-2-1, lower.tail = F)
```

R dispone di una funzione `chisq.test()`, ma non è adatta al test di normalità, infatti fornisce un risultato differente:

```{r}
chisq.test(sample)
```

Come si vede il risultato è completamente diverso. La funzione `chisq.test()`, infatti, serve in realtà ad un altro scopo, cioè l'analisi di **correlazione in tabelle di contingenza**.

Supponiamo di avere i dati che riportano quanti pezzi di una linea sono lavorati da uno di due operatori su ciascuna di tre macchine:

```{r}
set.seed(0)
df <- tibble(
  operatore = sample(2, 500, replace=T, prob=c(2,1)),
  macchina = sample(3, 500, replace=T, prob=c(3, 2.5, 1))
) %>% 
  mutate(operatore=factor(operatore), macchina=factor(macchina))
```

La tabella di contingenza mostra l'incidenza delle osservazione sulle due variabili categoriche:

```{r}
df %>% table()
```


Il test del Chi-quadro verifica l'ipotesi nulla che le due variabili categoriche siano indipendenti:

```{r}
chisq.test(table(df))
```
Verifichiamo l'opposto con una altra tabella di contingenza creata ad arte:

```{r}
m <- matrix(c(160, 140, 40, 40, 60, 60), nrow=2, byrow=T)
dimnames(m) <- list(operatore=1:2, macchina=1:3)
m
```

```{r}
chisq.test(m)
```


# Analisi della Varianza (ANOVA)

Consideriamo le prove a trazione su filati con diversi percentuali di cotone:

```{r}
df <- read.table(examples_url("cotton.dat"), header=T, comment.char="#", sep="\t") %>% 
  tibble() %>% 
  mutate(
    Cotton = factor(Cotton)
  )

df %>% 
  ggplot(aes(x=Cotton, y=Strength)) +
  geom_boxplot()
```

L'analisi della varianza parte dalla definizione di un **modello lineare** che correla la resa (output) con il fattore in ingresso (input) mediante una **formula R**:

```{r}
df.lm <- lm(Strength ~ Cotton, data=df)
anova(df.lm)
```

La formula `Strength~Cotton` corrisponde al **modello statistico lineare** $y_{ij} = \mu_i + \varepsilon_{ij}$. Dato un modello, la formula si ottiene eliminando i residui $\varepsilon_{ij}$ e sostituendo $=$ con `~`.

La tabella ANOVA conferma che **almeno uno dei trattamenti ha un effetto statisticamente significativo**.

Prima di accettare questa conclusione è però indispensabile effettuare l'analisi dei residui, verificando che siano normali:

```{r}
shapiro.test(df.lm$residuals)
```

Lasciamo **per esercizio** la verifica con Q-Q plot.

Le vecchie versioni di R utilizzavano una differente interfaccia per il calcolo della tabella ANOVA:

```{r}
aov(Strength~Cotton, data=df) %>% summary()
# oppure:
# summary(aov(Strength~Cotton, data=df))
```


## Test di Tukey

La funzione `aov()` è in realtà anche utile per realizzare il test di Tukey:

```{r}
df.tuk <- TukeyHSD(aov(Strength~Cotton, data=df), conf.level = 0.99)
df.tuk
```

È possibile ottenere un grafico direttamente con la funzione base `plot()`, ma non è molto leggibile:

```{r}
plot(df.tuk)
```

Per ottenere lo stesso grafico con `ggplot`:

```{r}
df.tuk$Cotton %>% 
  fortify() %>% # Per convertire in tibble
  rownames_to_column("id") %>% 
  ggplot(aes(x=id)) +
  geom_point(aes(y=diff)) +
  geom_hline(yintercept = 0, color="red") +
  geom_errorbar(aes(ymin=lwr, ymax=upr)) +
  coord_flip()
```

**Esercizio**: provare a cambiare il valore del livello di confidenza e discutere le variazioni conseguenti sul grafico.

```{r}
df.tuk <- TukeyHSD(aov(Strength~Cotton, data=df), conf.level = 0.90)
df.tuk$Cotton %>% 
  fortify() %>% # Per convertire in tibble
  rownames_to_column("id") %>% 
  ggplot(aes(x=id)) +
  geom_point(aes(y=diff)) +
  geom_hline(yintercept = 0, color="red") +
  geom_errorbar(aes(ymin=lwr, ymax=upr)) +
  coord_flip()
```
*Si osserva che riducendo il livello di confidenza dal 99% al 90% gli intervalli diventano **più piccoli**, cioè basta una difeferenza tra i campioni più piccola per rifiutare l'ipotesi nulla.*

## ANOVA a due vie

Consideriamo un esperimento che misura la vita di una batteria al variare della temperatura di esercizio e del tipo di elettrolita utilizzato all'interno della batteria stessa:

```{r}
df <- read.table(examples_url("battery.dat"), comment="#", header=T) %>% 
  mutate(
    Temperature = factor(Temperature),
    Material = factor(LETTERS[Material])
  )
df
```

Il modello lineare deve essere costruito prendendo in considerazione i due fattori **e anche la loro interazione**. Si noti l'algebra delle formule R:

```{r}
df.lm <- lm(Response ~ Material * Temperature, data=df)
# equivale a:
df.lm <- lm(Response ~ Material + Temperature + Material:Temperature, data=df)
```

La formula `Response ~ Temperature * Material` corrisponde al modello analitico:
$$
y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \varepsilon_{ijk} = \hat y_{ij} + \varepsilon_{ijk}
$$

Otteniamo l'analisi della varianza per il modello `df.lm`:

```{r}
anova(df.lm)
```

Da cui risultano essere significativi sia gli effetti dei fattori che la loro interazione.

Osserviamo i dati:

```{r}
df %>%
  # mutate(Temperature = as.numeric(Temperature)) %>% 
  group_by(Temperature, Material) %>% 
  summarise(Resp=mean(Response), min=min(Response), max=max(Response)) %>% 
  ggplot(aes(x=Temperature, y=Resp, color=Material, group=Material)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin=min, ymax=max), width=0.1, position = position_dodge(width=0.1))
```

Si noti l'uso di `position=position_dodge()`, che aggiunge un piccolo offset alla posizione orizzontale delle barre d'errore, in modo che non essendo sovrapposte risultino più leggibili.

**Esercizio**: Usare un test di Tukey (in realtà tre) per verificare quali valori di risposta sono statisticamente significativi **al 99%** ai diversi livelli di temperatura

Questo esercizio potete risolverlo usando un ciclo `for`. Di seguito mostro però come può essere risolto con un uso evoluto di `dplyr`, `purrr` e `ggplot`.

Comincio costruendomi una lista di oggetti `TukeyHSD`, usando i livelli del fattore `Temperature` come nomi degli elementi in lista:

```{r}
res <- map(levels(df$Temperature),
  ~ TukeyHSD(aov(Response~Material, data=filter(df, Temperature==.)), conf.level = 0.99)
)
names(res) <- levels(df$Temperature)
```

Ora `res` è una lista con tre elementi: `res[["15"]]`, `res[["70"]]` e `res[["125"]]`. Ciascun elemento è a sua volta una lista con un elemento, il quale contiene una matrice coi dati che ci interessano. Ad esempio:

```{r}
res[["15"]][[1]]
```

Voglio crearmi un'unica tibble che contenga tutte le tre matrici, con in più una colonna che rappresenti le temperature. Per farlo uso la funzione `purrr::reduce()`, che è analoga alla funzione `map()`, ma **accumula** gli elementi della lista su cui opera dentro un contenitore specificato dal parametro `.init`, che inizialmente impostiamo come una tibble vuota:

```{r}
names(res) %>% 
  reduce(                          # vuole una funzione a due valori
    function(tbl, key) {           # key di volta in volta è un livello di T
      bind_rows(tbl,               # tbl è l'accumulatore
        res[[key]][[1]] %>%        # estraggo il test per valore corrente di T
          fortify() %>%            # da aray in tibble
          rownames_to_column("id") %>% 
          add_column(T=key, .before="id") # aggiungo colonna con T (valori ripetuti)
      )
    }, 
    .init=tibble()                 # Valore iniziale dell'accumulatore
  ) %>% 
  mutate(Significant=`p adj` < 0.01)
```

**Nota**: per capire meglio come opera `reduce()` usiamo un esempio più semplice, accodando elementi di un vettore in una lista:

```{r}
LETTERS[1:3] %>% 
  reduce(
    function(lst, elem) {
      lst %>% append(elem)
    },
  .init=list()
  )
```
Come in tutte le funzioni di `purrr` posso sostutuire una funzione a due valori `function(a, b)` con `~` e usare `.x` come primo valore e `.y` come secondo, nel corpo della funzione:

```{r}
LETTERS[1:3] %>% 
  reduce(~ append(.x, .y), .init=list())
```

Ora posso realizzare il grafico:

```{r}
names(res) %>% 
  reduce(.init=tibble(), ~ {
    bind_rows(.x,
      res[[.y]][[1]] %>%
        fortify() %>%
        rownames_to_column("id") %>% 
        add_column(T=.y, .before="id")
    )}
  ) %>%
  mutate(Significant=`p adj` < 0.01) %>% 
  ggplot(aes(x=id, y=diff, group=T, color=T)) +
    geom_point(position=position_dodge(width=0.1)) +
    geom_hline(yintercept = 0, color="red") +
    geom_errorbar(aes(ymin=lwr, ymax=upr, linetype = Significant), 
                  width=0.2, 
                  position=position_dodge(width=0.1)) +
    scale_linetype_manual(values = c(2, 1))+
    coord_flip() +
  labs(y="Differenza di durata", x="Coppia di materiali", color="Temp.")
```

Dal grafico osserviamo che gli unici due casi in cui il materiale dielettrico comporta effettivamente una differenza significativa sul tempo di scarica delle batterie sono le differenze `B-A`e `C-A` a 70°C.

Si noti che è anche possibile osservare i dati grezzi con un box-plot a due variabili:

```{r}
df %>% 
  ggplot(aes(x=Temperature, y=Response, color=Material)) +
  geom_boxplot()
```
### Analisi dei residui

L'ultimo passo è rappresentato dall'analisi dei residui. Dato un modello lineare come `df.lm`, i residui si travano nel campo `df.lm$residuals`, oppure si possono ottenere con la funzione:

```{r}
residuals(df.lm)
```

Come si vede è un vettore con tanti elementi quanti il set di dati iniziali; ciascun elemento rapresenta la differenza tra osservazione e modello: $\varepsilon_{ijk} = y_{ijk} - \hat y_{ij}$

Per prima cosa verifichiamo la normalità con un test di Shapiro:

```{r}
df.lm$residuals %>% shapiro.test()
```

Il *p-value* elevato significa che possiamo accettare l'ipotesi nulla (normalità). Ricordiamo che il test di Shapiro è molto potente e, nel caso in cui il *p-value* fosse piccolo sarebbe comunque opportuno verificare con un grafico Q-Q. Non è il nostro caso, ma facciamolo comunque.

Per prima cosa aggiungiamo le predizioni e i residui al data frame originario dal modello `df.lm`, in modo da avere una tibble facile da trattare con `ggplot`. La libreria `modelr` (da installare) fornisce due utili funzioni che aggiungono al data frame la colonna coi residui $\varepsilon$ e quella con i valori predetti $\hat y$:

```{r}
df <- df %>% 
  add_residuals(df.lm) %>% 
  add_predictions(df.lm)
```

Mettiamo in grafico Q-Q i residui:

```{r}
df %>% 
  ggplot(aes(sample=resid)) +
  geom_qq() +
  geom_qq_line(color="red") +
  labs(x="Quantili teorici", y="Quantili campionari")
```

Dobbiamo infine verificare l'assenza di **pattern**, per escludere che effetti **ignoti e incontrollabili** modifichino la resa del l'esperimento col passare del tempo o per le diverse condizioni operative:

```{r}
df %>% 
  ggplot(aes(x=Temperature, y=resid, color=Material)) +
  geom_point()
```

```{r}
df %>% 
  ggplot(aes(x=Material, y=resid, color=Temperature)) +
  geom_point()
```
Quando le differenze sono inferiori a circa due volte l'estensione dei vari gruppi di punti (come in questo caso), significa che l'esperimento non presenta problemi.

Il data frame originario contiene anche la colonna `RunOrder`, che rappresenta l'ordine **casuale** in cui sono state effettuate le singole prove. Essendo l'ordine appunto casuale, è evidente che eventuali disturbi tempo-dipendenti si distribuiscono casualmente sulle varie condizioni di prova, aumentando la varianza ma in maniera non specifica. Ciò diminuisce la potenza del test (cioè ho bisogno di differenze più evidenti per rifiutare l'ipotesi nulla), ma non aumenta la possibilità di falsi positivi.

Un grafico dei residui contro l'ordine di esecuzione mi consente di evidenziare l'eventuale comparsa di effetti ignoti e non controllati, che se ci fossero tenderebbero ad aumentare o diminuire la variabilità dei residui in funzione del tempo:

```{r}
df %>% 
  ggplot(aes(x=RunOrder, y=resid)) +
  geom_point()
```

Anche in questo caso, il grafico non manifesta problemi, quindi **possiamo accettare l'analisi ANOVA sopra effettuata**.


